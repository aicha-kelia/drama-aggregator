#!/usr/bin/env python3
"""
FIXED Arabic Drama Scraper
Uses CORRECT selectors based on actual HTML structure
"""

import requests
from bs4 import BeautifulSoup
import json
import re
import time
from datetime import datetime
from typing import List, Dict, Optional
import unicodedata


class DramaScraper:
    def __init__(self):
        self.scraped_dramas = []
        self.seen_titles = set()
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ar,en-US;q=0.7,en;q=0.3',
        }
        
        # Genre mapping
        self.genre_map = {
            'Ø±ÙˆÙ…Ø§Ù†Ø³ÙŠ': 'romance',
            'ÙƒÙˆÙ…ÙŠØ¯ÙŠ': 'comedy',
            'Ø¯Ø±Ø§Ù…Ø§': 'drama',
            'Ø¥Ø«Ø§Ø±Ø©': 'thriller',
            'ØºÙ…ÙˆØ¶': 'mystery',
            'Ø¬Ø±ÙŠÙ…Ø©': 'crime',
            'ØªØ§Ø±ÙŠØ®ÙŠ': 'historical',
            'ÙØ§Ù†ØªØ§Ø²ÙŠØ§': 'fantasy',
            'Ø£ÙƒØ´Ù†': 'action',
            'Ø±Ø¹Ø¨': 'horror',
            'Ø·Ø¨ÙŠ': 'medical',
        }
        
        # Country mapping
        self.country_map = {
            'ÙƒÙˆØ±ÙŠ': 'korean',
            'ÙƒÙˆØ±ÙŠØ§': 'korean',
            'ØµÙŠÙ†ÙŠ': 'chinese',
            'Ø§Ù„ØµÙŠÙ†': 'chinese',
            'ØªØ§ÙŠÙˆØ§Ù†': 'taiwanese',
            'ØªØ±ÙƒÙŠ': 'turkish',
            'ØªØ±ÙƒÙŠØ§': 'turkish',
            'ÙŠØ§Ø¨Ø§Ù†ÙŠ': 'japanese',
            'Ø§Ù„ÙŠØ§Ø¨Ø§Ù†': 'japanese',
            'ØªØ§ÙŠÙ„Ù†Ø¯ÙŠ': 'thai',
            'ØªØ§ÙŠÙ„Ù†Ø¯': 'thai',
        }
    
    def normalize_title(self, title: str) -> str:
        """Normalize title for duplicate detection"""
        title = ''.join(c for c in unicodedata.normalize('NFD', title)
                       if unicodedata.category(c) != 'Mn')
        title = re.sub(r'[^\w\s]', '', title.lower()).strip()
        title = ' '.join(title.split())
        return title
    
    def is_duplicate(self, title: str, year: int, country: str) -> bool:
        """Check if drama already exists"""
        key = f"{self.normalize_title(title)}_{year}_{country}"
        if key in self.seen_titles:
            return True
        self.seen_titles.add(key)
        return False
    
    def extract_year(self, text: str) -> int:
        """Extract year from text"""
        match = re.search(r'(20\d{2})', text)
        return int(match.group(1)) if match else datetime.now().year
    
    def map_genre(self, arabic_genres: List[str]) -> List[str]:
        """Map Arabic genres to English"""
        english_genres = []
        for genre in arabic_genres:
            genre = genre.strip()
            if genre in self.genre_map:
                english_genres.append(self.genre_map[genre])
        return english_genres if english_genres else ['drama']
    
    def map_country(self, text: str) -> str:
        """Map Arabic country to English"""
        text_lower = text.lower()
        for arabic, english in self.country_map.items():
            if arabic in text_lower:
                return english
        return 'unknown'
    
    def scrape_aradrama_tv(self, max_dramas: int = 100):
        """Scrape ArabDrama TV using CORRECT selectors"""
        print("\nğŸ” Scraping ArabDrama TV...")
        
        base_url = "https://aradramatv.cc"
        
        # Scrape multiple pages
        categories = [
            ('/category/serie/korea/', 'korean'),
            ('/category/serie/chinese-taiwan/', 'chinese'),
            ('/category/serie/japanese/', 'japanese'),
        ]
        
        drama_count = 0
        
        for category_url, default_country in categories:
            if drama_count >= max_dramas:
                break
                
            print(f"\n  ğŸ“‚ Category: {category_url}")
            
            # Scrape multiple pages in each category
            for page in range(1, 6):  # Pages 1-5
                if drama_count >= max_dramas:
                    break
                
                try:
                    if page == 1:
                        url = f"{base_url}{category_url}"
                    else:
                        url = f"{base_url}{category_url}page/{page}/"
                    
                    print(f"    â†’ Page {page}: {url}")
                    
                    response = requests.get(url, headers=self.headers, timeout=15)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # CORRECT SELECTOR: Find all post-row divs
                    drama_cards = soup.find_all('div', class_='post-row')
                    
                    print(f"      Found {len(drama_cards)} drama cards")
                    
                    for card in drama_cards:
                        if drama_count >= max_dramas:
                            break
                        
                        try:
                            # Extract drama link
                            link = card.find('a')
                            if not link:
                                continue
                            
                            drama_url = link.get('href', '')
                            if not drama_url.startswith('http'):
                                drama_url = base_url + drama_url
                            
                            # Extract title from b_title div
                            title_div = card.find('div', class_='b_title')
                            if not title_div:
                                continue
                            
                            title_text = title_div.get_text(strip=True)
                            
                            # Extract thumbnail
                            img = card.find('img')
                            thumbnail = img.get('src', '') if img else ''
                            if thumbnail and not thumbnail.startswith('http'):
                                thumbnail = base_url + thumbnail
                            
                            # Extract filter info (genre/year)
                            filter_div = card.find('div', class_='filter')
                            filter_text = filter_div.get_text(strip=True) if filter_div else ''
                            
                            # Now visit the drama page for full details
                            drama_data = self.scrape_drama_page(
                                drama_url, 
                                title_text, 
                                thumbnail,
                                filter_text,
                                default_country
                            )
                            
                            if drama_data:
                                if not self.is_duplicate(
                                    drama_data['title'],
                                    drama_data['release_year'],
                                    drama_data['country']
                                ):
                                    self.scraped_dramas.append(drama_data)
                                    drama_count += 1
                                    print(f"      âœ… [{drama_count}] {drama_data['title_arabic'][:50]}")
                                else:
                                    print(f"      â­ï¸  Duplicate: {title_text[:30]}")
                            
                            time.sleep(0.5)
                            
                        except Exception as e:
                            print(f"      âš ï¸  Error processing card: {e}")
                            continue
                    
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"    âŒ Error on page {page}: {e}")
                    continue
        
        print(f"\nâœ… Scraped {drama_count} dramas from ArabDrama TV")
    
    def scrape_drama_page(self, url: str, title: str, thumbnail: str, filter_text: str, default_country: str) -> Optional[Dict]:
        """Scrape individual drama page for full details"""
        try:
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Initialize data
            drama_data = {
                'title': '',
                'title_arabic': '',
                'title_original': '',
                'description': '',
                'description_arabic': '',
                'country': default_country,
                'total_episodes': 0,
                'episode_duration': 60,
                'release_year': datetime.now().year,
                'status': 'completed',
                'genres': ['drama'],
                'thumbnail_url': thumbnail,
                'watch_links': [{
                    'website_name': 'ArabDrama TV',
                    'url': url,
                    'language': 'arabic',
                    'episodes_available': 0
                }],
                'scraped_from_site': 'ArabDrama TV',
            }
            
            # Parse title (format: "Arabic Title English Title" or just Arabic)
            title = title.strip()
            
            # Check if title contains both Arabic and English
            # Split by common patterns
            if ' - ' in title:
                parts = title.split(' - ', 1)
                drama_data['title_arabic'] = parts[0].strip()
                drama_data['title'] = parts[1].strip()
            elif re.search(r'[\u0600-\u06FF]', title):
                # Has Arabic characters
                drama_data['title_arabic'] = title
                drama_data['title'] = title
            else:
                drama_data['title'] = title
                drama_data['title_arabic'] = title
            
            # Extract info from page content
            content_div = soup.find('div', class_='entry-content')
            if content_div:
                content_text = content_div.get_text()
                
                # Extract year
                year = self.extract_year(content_text)
                drama_data['release_year'] = year
                
                # Extract episodes
                ep_match = re.search(r'Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù„Ù‚Ø§Øª[:\s]*(\d+)', content_text)
                if ep_match:
                    drama_data['total_episodes'] = int(ep_match.group(1))
                    drama_data['watch_links'][0]['episodes_available'] = drama_data['total_episodes']
                
                # Extract genre
                genre_match = re.search(r'Ø§Ù„Ù†ÙˆØ¹[:\s]*([^\n]+)', content_text)
                if genre_match:
                    genres_text = genre_match.group(1).strip()
                    genres_list = [g.strip() for g in re.split(r'[ØŒ,/]', genres_text)]
                    drama_data['genres'] = self.map_genre(genres_list)
                
                # Extract country
                country_match = re.search(r'Ø§Ù„Ø¨Ù„Ø¯[:\s]*([^\n]+)', content_text)
                if country_match:
                    drama_data['country'] = self.map_country(country_match.group(1))
                
                # Extract description (Ø§Ù„Ù‚ØµØ©)
                desc_match = re.search(r'Ø§Ù„Ù‚ØµØ©[:\s]*([^\n]{50,})', content_text)
                if desc_match:
                    drama_data['description_arabic'] = desc_match.group(1).strip()
                    drama_data['description'] = drama_data['description_arabic']
                
                # Status
                if 'Ù…Ø³ØªÙ…Ø±' in content_text or 'ÙŠØ¹Ø±Ø¶' in content_text:
                    drama_data['status'] = 'ongoing'
            
            return drama_data
            
        except Exception as e:
            print(f"        âš ï¸  Error scraping page: {e}")
            return None
    
    def save_to_json(self, filename: str = 'scraped_dramas.json'):
        """Save to JSON"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.scraped_dramas, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ Saved {len(self.scraped_dramas)} dramas to {filename}")
        return filename
    
    def print_summary(self):
        """Print summary"""
        print(f"\n" + "="*60)
        print(f"ğŸ“Š SCRAPING SUMMARY")
        print(f"="*60)
        print(f"Total dramas: {len(self.scraped_dramas)}")
        
        # By country
        countries = {}
        for drama in self.scraped_dramas:
            country = drama['country']
            countries[country] = countries.get(country, 0) + 1
        
        print(f"\nğŸ“ By Country:")
        for country, count in sorted(countries.items(), key=lambda x: x[1], reverse=True):
            print(f"  {country}: {count}")
        
        # By year
        years = {}
        for drama in self.scraped_dramas:
            year = drama['release_year']
            years[year] = years.get(year, 0) + 1
        
        print(f"\nğŸ“… By Year:")
        for year, count in sorted(years.items(), reverse=True):
            print(f"  {year}: {count}")
        
        print(f"="*60)


def main():
    print("ğŸ¬ ARABIC DRAMA SCRAPER (FIXED VERSION)")
    print("="*60)
    
    scraper = DramaScraper()
    
    # Scrape dramas (adjust max_dramas as needed)
    scraper.scrape_aradrama_tv(max_dramas=100)  # Change to 500 for more
    
    # Print summary
    scraper.print_summary()
    
    # Save to JSON
    output_file = scraper.save_to_json('scraped_dramas_FIXED.json')
    
    print(f"\nâœ… DONE!")
    print(f"\nğŸ’¡ NEXT: Import this file into Django:")
    print(f"   python manage.py import_dramas {output_file}")


if __name__ == '__main__':
    main()